{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LSoGDNUH3310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 16:06:10.906623: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-02 16:06:11.304843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-02 16:06:11.304943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-02 16:06:11.376623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-02 16:06:11.519201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-02 16:06:12.332549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, SimpleRNN, Embedding, Dense, Attention\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load your dataset\n",
    "base_dir = 'datasets/mt/'\n",
    "train_data = pd.read_csv(base_dir + 'train.csv')\n",
    "val_data = pd.read_csv(base_dir + 'valid.csv')\n",
    "test_data = pd.read_csv(base_dir + 'test.csv')\n",
    "# Ensure your dataset has 'source' and 'target' columns\n",
    "train_source_texts = train_data['indonesian'].values\n",
    "train_target_texts = train_data['english'].values\n",
    "\n",
    "val_source_texts = val_data['indonesian'].values\n",
    "val_target_texts = val_data['english'].values\n",
    "\n",
    "test_source_texts = test_data['indonesian'].values\n",
    "test_target_texts = test_data['english'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.15.0\n",
      "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
      "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.12.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.67.0)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.44.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.5)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.0)\n",
      "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: wrapt, tensorflow-estimator, pyasn1, protobuf, numpy, keras, cachetools, rsa, requests-oauthlib, pyasn1-modules, ml-dtypes, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.6.0\n",
      "    Uninstalling keras-3.6.0:\n",
      "      Successfully uninstalled keras-3.6.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml-dtypes 0.4.1\n",
      "    Uninstalling ml-dtypes-0.4.1:\n",
      "      Successfully uninstalled ml-dtypes-0.4.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed cachetools-5.5.0 google-auth-2.35.0 google-auth-oauthlib-1.2.1 keras-2.15.0 ml-dtypes-0.2.0 numpy-1.26.4 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2\n",
      "8\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.sysconfig.get_build_info()['cuda_version'])\n",
    "print(tf.sysconfig.get_build_info()['cudnn_version'])\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('cpu_compiler', '/usr/lib/llvm-17/bin/clang'),\n",
       "             ('cuda_compute_capabilities',\n",
       "              ['sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']),\n",
       "             ('cuda_version', '12.2'),\n",
       "             ('cudnn_version', '8'),\n",
       "             ('is_cuda_build', True),\n",
       "             ('is_rocm_build', False),\n",
       "             ('is_tensorrt_build', True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sysconfig.get_build_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "erDDGIBR3314"
   },
   "outputs": [],
   "source": [
    "# Tokenization (Source and Target)\n",
    "source_tokenizer = Tokenizer()\n",
    "target_tokenizer = Tokenizer()\n",
    "\n",
    "source_tokenizer.fit_on_texts(train_source_texts)\n",
    "target_tokenizer.fit_on_texts(train_target_texts)\n",
    "\n",
    "source_vocab_size = len(source_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Convert texts to sequences\n",
    "train_source_sequences = source_tokenizer.texts_to_sequences(train_source_texts)\n",
    "train_target_sequences = target_tokenizer.texts_to_sequences(train_target_texts)\n",
    "\n",
    "val_source_sequences = source_tokenizer.texts_to_sequences(val_source_texts)\n",
    "val_target_sequences = target_tokenizer.texts_to_sequences(val_target_texts)\n",
    "\n",
    "test_source_sequences = source_tokenizer.texts_to_sequences(test_source_texts)\n",
    "test_target_sequences = target_tokenizer.texts_to_sequences(test_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GP9MHKOL3315",
    "outputId": "4ba3d627-52a6-48ee-e272-d22feeaa9b93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(train_target_sequences[1]))\n",
    "print(len(val_target_sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b9isI-Qg3317"
   },
   "outputs": [],
   "source": [
    "# Pad sequences to ensure uniform input size\n",
    "max_source_length = 77\n",
    "max_target_length = 77   #to include EOS or shifted tokens\n",
    "X_train_source = pad_sequences(train_source_sequences, maxlen=max_source_length, padding='post')\n",
    "X_train_target = pad_sequences(train_target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "X_val_source = pad_sequences(val_source_sequences, maxlen=max_source_length, padding='post')\n",
    "X_val_target = pad_sequences(val_target_sequences, maxlen=max_target_length, padding='post')\n",
    "\n",
    "X_test_source = pad_sequences(test_source_sequences, maxlen=max_source_length, padding='post')\n",
    "X_test_target = pad_sequences(test_target_sequences, maxlen=max_target_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MYpNUmna3318"
   },
   "outputs": [],
   "source": [
    "# Shift target sequences for teacher forcing\n",
    "train_target_sequences_shifted = np.zeros_like(X_train_target)\n",
    "train_target_sequences_shifted[:, :-1] = X_train_target[:, 1:]\n",
    "\n",
    "# One-hot encode target sequences\n",
    "y_train = X_train_target[:, 1:]\n",
    "y_train = to_categorical(train_target_sequences_shifted, num_classes=target_vocab_size)\n",
    "\n",
    "val_target_sequences_shifted = np.zeros_like(X_val_target)\n",
    "val_target_sequences_shifted[:, :-1] = X_val_target[:, 1:]\n",
    "\n",
    "y_val = X_val_target[:, 1:]  # Exclude the first token\n",
    "y_val = to_categorical(val_target_sequences_shifted, num_classes=target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLWrfTYv3319",
    "outputId": "62042709-3d75-46c8-ec11-24f677b427bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_source shape: (500, 77)\n",
      "X_val_source shape: (100, 77)\n",
      "y_train shape: (500, 77, 2863)\n",
      "y_val shape: (100, 77, 2863)\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions\n",
    "print(f'X_train_source shape: {X_train_source.shape}')\n",
    "print(f'X_val_source shape: {X_val_source.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_val shape: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fy-2e-TR332A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 16:24:14.012085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.012391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.012604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.107731: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.107987: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.108170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-02 16:24:14.108308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5935 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-11-02 16:24:14.572710: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_SIZE = 256       # Size of the embedding layer\n",
    "RNN_UNITS = 256            # Number of RNN units\n",
    "max_source_length = 77    # Max length of source sequences\n",
    "max_target_length = 77    # Max length of target sequences\n",
    "\n",
    "# Encoder Model\n",
    "encoder_inputs = Input(shape=(max_source_length,))\n",
    "encoder_embedding_layer = Embedding(source_vocab_size, EMBEDDING_SIZE)(encoder_inputs)\n",
    "encoder = SimpleRNN(RNN_UNITS, return_state=True)\n",
    "encoder_outputs, h = encoder(encoder_embedding_layer)\n",
    "encoder_states = h\n",
    "\n",
    "# Decoder Model\n",
    "decoder_inputs = Input(shape=(1,))  # Input shape for a single time step\n",
    "decoder_embedding_layer = Embedding(target_vocab_size, EMBEDDING_SIZE)(decoder_inputs)\n",
    "decoder = SimpleRNN(RNN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder(decoder_embedding_layer, initial_state=encoder_states)\n",
    "\n",
    "# Output Layer\n",
    "output_dense_layer = Dense(target_vocab_size, activation='softmax')\n",
    "outputs = output_dense_layer(decoder_outputs)\n",
    "\n",
    "# Complete Seq2Seq Model\n",
    "model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF_KxXDG332B",
    "outputId": "03153f49-0f04-4c1e-b84f-f455722330ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 256)\n",
      "(None, 1, 2863)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_outputs.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Xaf04Dlm332D"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# Add model checkpoint to save the best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'mt_best_val_accuracy_{val_accuracy:.4f}.keras',  # File path to save the model\n",
    "    monitor='val_accuracy',  # Monitor validation loss\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    mode='max',  # Minimize validation loss\n",
    "    verbose=1  # Print a message when the model is saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIs3LN0M332E",
    "outputId": "d52876f1-999d-480b-d79e-b46f77f6e112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the ModelCheckpoint callback\n",
    "history = model.fit([X_train_source, X_train_target], y_train,\n",
    "                    epochs=50, batch_size=64,\n",
    "                    validation_data=([X_val_source, X_val_target], y_val),\n",
    "                    callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "UBW3pQcTKtei",
    "outputId": "8e975277-70c4-415c-f14f-364c459c53c2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve accuracy and loss from the history object\n",
    "history_dict = history.history\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbAWz7jCg-xy",
    "outputId": "4a301af8-3a90-4dd6-9a17-435f2b1767bb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('mt_best_val_accuracy_0.7217.keras')\n",
    "\n",
    "# Generate predictions\n",
    "predicted_probabilities = model.predict([X_val_source, X_val_target])\n",
    "predicted_sequences = np.argmax(predicted_probabilities, axis=-1)\n",
    "\n",
    "# Convert predictions and references to lists of sentences\n",
    "predicted_sentences = []\n",
    "reference_sentences = []\n",
    "\n",
    "for i in range(len(predicted_sequences)):\n",
    "    predicted_sent = predicted_sequences[i]\n",
    "    reference_sent = X_test_target[i]\n",
    "\n",
    "    # Remove padding (assuming padding is done with a specific token, e.g., 0)\n",
    "    predicted_sent = [word for word in predicted_sent if word != 0]\n",
    "    reference_sent = [word for word in reference_sent if word != 0]\n",
    "\n",
    "    predicted_sentences.append(predicted_sent)\n",
    "    reference_sentences.append([reference_sent])  # NLTK expects a list of references\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = corpus_bleu(reference_sentences, predicted_sentences)\n",
    "print(f'BLEU Score: {bleu_score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsPbusPF332F",
    "outputId": "a2499d11-d0a1-4c2d-9a8f-d6f524c03c1e"
   },
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "test_target_sequences_shifted = np.zeros_like(X_test_target)\n",
    "test_target_sequences_shifted[:, :-1] = X_test_target[:, 1:]\n",
    "y_test = to_categorical(test_target_sequences_shifted, num_classes=target_vocab_size)\n",
    "test_loss, test_accuracy = model.evaluate([X_test_source, X_test_target], y_test)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQLCdG86aUwK"
   },
   "outputs": [],
   "source": [
    "word2idx_inputs = source_tokenizer.word_index\n",
    "word2idx_outputs = target_tokenizer.word_index\n",
    "\n",
    "# Check the current max index in word2idx_outputs\n",
    "max_index = max(word2idx_outputs.values())\n",
    "\n",
    "# Add the <sos> token with the next available index\n",
    "word2idx_outputs['<sos>'] = max_index + 1\n",
    "\n",
    "# Optional: Ensure idx2word_target is updated accordingly\n",
    "idx2word_target = {v: k for k, v in word2idx_outputs.items()}\n",
    "\n",
    "\n",
    "idx2word_input = {v:k for k, v in word2idx_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3GKVTi8cIF3",
    "outputId": "827fced5-821a-4361-dc59-5959aad2152f"
   },
   "outputs": [],
   "source": [
    "word2idx_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYCxdgPeMHPP",
    "outputId": "4959e9a9-2795-41fb-c4c6-e25dd4d3b6fb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Hyperparameters (same as training)\n",
    "EMBEDDING_SIZE = 256       # Size of the embedding layer\n",
    "RNN_UNITS = 256            # Number of RNN units\n",
    "max_source_length = 77      # Max length of source sequences\n",
    "max_target_length = 77      # Max length of target sequences\n",
    "\n",
    "# --- Encoder Inference Model ---\n",
    "\n",
    "# Encoder inputs (same shape as training)\n",
    "encoder_inputs = Input(shape=(max_source_length,))\n",
    "encoder_embedding_layer = Embedding(source_vocab_size, EMBEDDING_SIZE)(encoder_inputs)\n",
    "encoder_rnn = SimpleRNN(RNN_UNITS, return_state=True)\n",
    "\n",
    "# Get encoder's hidden state\n",
    "encoder_outputs, encoder_state_h = encoder_rnn(encoder_embedding_layer)\n",
    "encoder_model = Model(encoder_inputs, encoder_state_h)  # Encoder model returns hidden state\n",
    "\n",
    "# --- Decoder Inference Model ---\n",
    "\n",
    "# Decoder inputs (single time step input for inference)\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_state_input_h = Input(shape=(RNN_UNITS,))  # Hidden state input for inference\n",
    "\n",
    "# Embedding layer\n",
    "decoder_embedding_layer = Embedding(target_vocab_size, EMBEDDING_SIZE)(decoder_inputs_single)\n",
    "\n",
    "# RNN layer (with previous hidden state as initial state)\n",
    "decoder_rnn = SimpleRNN(RNN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_h = decoder_rnn(decoder_embedding_layer, initial_state=decoder_state_input_h)\n",
    "\n",
    "# Dense softmax layer to predict the next token\n",
    "output_dense_layer = Dense(target_vocab_size, activation='softmax')\n",
    "decoder_outputs = output_dense_layer(decoder_outputs)\n",
    "\n",
    "# Define the decoder inference model, which outputs predicted token and new hidden state\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single, decoder_state_input_h],\n",
    "    [decoder_outputs, decoder_state_h]\n",
    ")\n",
    "\n",
    "# --- Function to Generate Translations ---\n",
    "\n",
    "def translate_sentence(input_seq):\n",
    "    # Encode the input sequence to get the initial hidden state\n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Initialize the target sequence with the start token <sos>\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']  # Or another start token\n",
    "\n",
    "    output_sentence = []\n",
    "    stop_condition = False\n",
    "    while not stop_condition:\n",
    "        # Predict the next token and the next hidden state\n",
    "        output_tokens, h = decoder_model.predict([target_seq, state_value])\n",
    "\n",
    "        # Get the index of the predicted token\n",
    "        predicted_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        predicted_word = idx2word_target.get(predicted_token_index, '')\n",
    "\n",
    "        # Append predicted word to the output sentence\n",
    "        if predicted_word == '<eos>' or len(output_sentence) >= max_target_length:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            output_sentence.append(predicted_word)\n",
    "\n",
    "        # Update the target sequence (current predicted token becomes next input)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = predicted_token_index\n",
    "\n",
    "        # Update the hidden state for the next time step\n",
    "        state_value = h\n",
    "\n",
    "    return ' '.join(output_sentence)\n",
    "\n",
    "# Test the translation with an example input sentence\n",
    "input_seq = X_test_source[0:1]  # Example input\n",
    "translated_sentence = translate_sentence(input_seq)\n",
    "# Assuming X_test_source is a numpy array or a list containing your test sentences\n",
    "# You may want to iterate through the input sequences and print them\n",
    "\n",
    "for i, input_sentence in enumerate(input_seq):\n",
    "    # If the input is tokenized (numerical indices), you may want to reverse it to words\n",
    "    input_sentence_words = ' '.join(idx2word_input.get(token, '') for token in input_sentence if token != 0)  # Assuming 0 is padding\n",
    "    print(f'Input Sentence {i + 1}: {input_sentence_words}')\n",
    "\n",
    "print('Predicted Translation:', translated_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
